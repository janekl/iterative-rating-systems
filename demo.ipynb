{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from rating_models import IterativeMargin\n",
    "from optimize_model import train_valid_test_split\n",
    "from utils.data import get_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.evaluation import logloss, brier, rps, accuracy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Margin model\n",
    "\n",
    "This notebook presents how to use iterative version of one-parameter Poisson model which proved to be the best performing approach in the experiments. Its name stems from the fact that the updates are driven by the difference between expected and actual goal differences, or win margin, in a match (analogously as the difference between expected and actual match result in the Elo model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'c': 0.02, 'h': 0.3, 'lr': 0.01, 'lambda_reg': 0.0001, 'goal_cap': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IterativeMargin(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_train, seasons_valid, seasons_test, seasons_all = train_valid_test_split('E', False)\n",
    "matches = get_data(seasons_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.fit_predict(matches, seasons_train, seasons_valid, seasons_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['E_1516', 'E_1617', 'E_1718', 'E_1819']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = matches['Season'].isin(seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = matches.loc[subset, 'FTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in (logloss, brier, rps, accuracy):\n",
    "    print('{:>10}: {:.4f}'.format(metric.__name__, metric(predictions[subset], results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Big six\" Premier League  ratings\n",
    "\n",
    "Remark: `ratings_history` attribute was added specifically for the purpose of retrieving history of ratings only for `IterativeMargin` model. It is not implemented in other models (though very easy to add)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.ratings_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.DataFrame(model.ratings_history, columns=['rating_home', 'rating_away'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.concat([matches, ratings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratings_team = defaultdict(list)\n",
    "\n",
    "for _, match in matches.iterrows():\n",
    "    team_home, team_away, rating_home, rating_away = match[['HomeTeam', 'AwayTeam', 'rating_home', 'rating_away']]\n",
    "    ratings_team[team_home].append(rating_home)\n",
    "    ratings_team[team_away].append(rating_away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_subset = ('Arsenal', 'Chelsea', 'Liverpool', 'Man City', 'Man United', 'Tottenham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seasons = 3\n",
    "num_rounds = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ratings for the last three seasons\n",
    "ratings_top = pd.DataFrame({team: ratings_team[team][-num_rounds*num_seasons:] for team in team_subset}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling team colors for the top two teams: Liverpool and Manchester City to align with their shirts\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "team_colors = [cycle[i] for i in [2, 1, 3, 0, 5, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_ticks = np.array([0, 9, 19, 29])\n",
    "save_fig = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ratings_top.plot(style='.-', figsize=(12, 6), lw=1.75, color=team_colors)\n",
    "plt.legend(fontsize=13)\n",
    "ax.set_xlabel('Rounds', fontsize=13)\n",
    "ax.set_ylabel('Ratings', fontsize=13)\n",
    "ax.text(14, 0.245, \"2016/17\", fontsize=11)\n",
    "ax.text(52, 0.245, \"2017/18\", fontsize=11)\n",
    "ax.text(90, 0.245, \"2018/19\", fontsize=11)\n",
    "for vline in [37.5, 38 + 37.5]:\n",
    "    plt.axvline(x=vline, c='k', ls=\":\", lw=1.0)\n",
    "\n",
    "plt.xticks(ticks=np.hstack([rounds_ticks, rounds_ticks + 38, rounds_ticks + 2 * 38]),\n",
    "           labels=np.hstack([rounds_ticks + 1] * len(rounds_ticks)), fontsize=12)\n",
    "\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "if save_fig:\n",
    "    ax.get_figure().savefig(\"ratings_top.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
